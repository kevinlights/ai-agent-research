{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "ollama pull llama3.2:1b\n",
    "\n",
    "pip install 'litellm[proxy]'\n",
    "litellm --model ollama/llama3.2:1b\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "from autogen_core import (\n",
    "    AgentId,\n",
    "    DefaultTopicId,\n",
    "    MessageContext,\n",
    "    RoutedAgent,\n",
    "    SingleThreadedAgentRuntime,\n",
    "    default_subscription,\n",
    "    message_handler,\n",
    ")\n",
    "from autogen_core.model_context import BufferedChatCompletionContext\n",
    "from autogen_core.models import (\n",
    "    AssistantMessage,\n",
    "    ChatCompletionClient,\n",
    "    SystemMessage,\n",
    "    UserMessage,\n",
    ")\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_client() -> OpenAIChatCompletionClient:  # type: ignore\n",
    "    \"Mimic OpenAI API using Local LLM Server.\"\n",
    "    return OpenAIChatCompletionClient(\n",
    "        model=\"ollama/llama3.2:1b\",\n",
    "        api_key=\"NotRequiredSinceWeAreLocal\",\n",
    "        base_url=\"http://0.0.0.0:4000\",\n",
    "        model_capabilities={\n",
    "            \"json_output\": False,\n",
    "            \"vision\": False,\n",
    "            \"function_calling\": True,\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from models import qwen_25_1_5b, llama3_2_3b\n",
    "# def get_model_client() -> OpenAIChatCompletionClient: \n",
    "#     return llama3_2_3b()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Message:\n",
    "    content: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@default_subscription\n",
    "class Assistant(RoutedAgent):\n",
    "    def __init__(self, name: str, model_client: ChatCompletionClient) -> None:\n",
    "        super().__init__(\"An assistant agent.\")\n",
    "        self._model_client = model_client\n",
    "        self.name = name\n",
    "        self.count = 0\n",
    "        self._system_messages = [\n",
    "            SystemMessage(\n",
    "                content=f\"Your name is {name} and you are a part of a duo of comedians.\"\n",
    "                \"You laugh when you find the joke funny, else reply 'I need to go now'.\",\n",
    "            )\n",
    "        ]\n",
    "        self._model_context = BufferedChatCompletionContext(buffer_size=5)\n",
    "\n",
    "    @message_handler\n",
    "    async def handle_message(self, message: Message, ctx: MessageContext) -> None:\n",
    "        self.count += 1\n",
    "        await self._model_context.add_message(UserMessage(content=message.content, source=\"user\"))\n",
    "        result = await self._model_client.create(self._system_messages + await self._model_context.get_messages())\n",
    "\n",
    "        print(f\"\\n{self.name}: {message.content}\")\n",
    "\n",
    "        if \"I need to go\".lower() in message.content.lower() or self.count > 2:\n",
    "            return\n",
    "\n",
    "        await self._model_context.add_message(AssistantMessage(content=result.content, source=\"assistant\"))  # type: ignore\n",
    "        await self.publish_message(Message(content=result.content), DefaultTopicId())  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime = SingleThreadedAgentRuntime()\n",
    "\n",
    "cathy = await Assistant.register(\n",
    "    runtime,\n",
    "    \"cathy\",\n",
    "    lambda: Assistant(name=\"Cathy\", model_client=get_model_client()),\n",
    ")\n",
    "\n",
    "joe = await Assistant.register(\n",
    "    runtime,\n",
    "    \"joe\",\n",
    "    lambda: Assistant(name=\"Joe\", model_client=get_model_client()),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w0/y_fylk8d1g5474hmb49dt1jr0000gn/T/ipykernel_2613/172301977.py:20: UserWarning: Resolved model mismatch: llama3.2:1b != ollama/llama3.2:1b. Model mapping in autogen_ext.models.openai may be incorrect. Set the model to ollama/llama3.2:1b to enhance token/cost estimation and suppress this warning.\n",
      "  result = await self._model_client.create(self._system_messages + await self._model_context.get_messages())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Joe: Joe, tell me a joke.\n",
      "\n",
      "Cathy: (laughs) Oh man, here's one: Why couldn't the bicycle stand up by itself? (pauses for comedic effect) Because it was two-tired! (laughs) I mean, that's just ridiculous. How's that, am I making you laugh or what?\n",
      "\n",
      "Joe: (laughs) Ahahaha, that's a good one, buddy! I think we're laughing at the absurdity of the situation. The pun on \"two-tired\" is brilliant. You know, I love it when comedians can take something as mundane as a bicycle and turn it into a hilarious joke. Keep 'em coming!\n",
      "\n",
      "Cathy: (laughs) Ahahah, nice try, buddy! That joke is a bit \"flat\" if you know what I mean. But hey, at least you tried to pedal your way into the audience with that one. What do you think? Should we \"chain\" our next jokes together for some laughs?\n",
      "\n",
      "Joe: (laughs) Oh man, I'm glad you're enjoying it! I've got another one: Why don't eggs tell jokes? (pauses for comedic effect) Because they'd crack each other up! (laughs) Get it? Crack? Like the shell? Ah, never mind...\n"
     ]
    }
   ],
   "source": [
    "runtime.start()\n",
    "await runtime.send_message(\n",
    "    Message(\"Joe, tell me a joke.\"),\n",
    "    # Message(\"Joe, 给我讲个笑话。\"),\n",
    "    recipient=AgentId(joe, \"default\"),\n",
    "    sender=AgentId(cathy, \"default\"),\n",
    ")\n",
    "await runtime.stop_when_idle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "open_manus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
