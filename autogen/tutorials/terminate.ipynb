{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- user ----------\n",
      "Write a unique, Haiku about the weather in Paris\n",
      "---------- primary ----------\n",
      "Rain kisses cobblestones,\n",
      "Eiffel shimmers through soft drizzle,\n",
      "Paris whispers wet.\n",
      "[Prompt tokens: 31, Completion tokens: 21]\n",
      "---------- critic ----------\n",
      "Your Haiku captures the essence of Paris well. Consider adding a touch that contrasts the cool dampness with the city's warmth and liveliness to make it even more vivid. APPROVE\n",
      "[Prompt tokens: 67, Completion tokens: 38]\n",
      "---------- Summary ----------\n",
      "Number of messages: 3\n",
      "Finish reason: Maximum number of messages 3 reached, current message count: 3\n",
      "Total prompt tokens: 98\n",
      "Total completion tokens: 59\n",
      "Duration: 3.19 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TaskResult(messages=[TextMessage(source='user', models_usage=None, metadata={}, content='Write a unique, Haiku about the weather in Paris', type='TextMessage'), TextMessage(source='primary', models_usage=RequestUsage(prompt_tokens=31, completion_tokens=21), metadata={}, content='Rain kisses cobblestones,\\nEiffel shimmers through soft drizzle,\\nParis whispers wet.', type='TextMessage'), TextMessage(source='critic', models_usage=RequestUsage(prompt_tokens=67, completion_tokens=38), metadata={}, content=\"Your Haiku captures the essence of Paris well. Consider adding a touch that contrasts the cool dampness with the city's warmth and liveliness to make it even more vivid. APPROVE\", type='TextMessage')], stop_reason='Maximum number of messages 3 reached, current message count: 3')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "call approve function\n"
     ]
    }
   ],
   "source": [
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.conditions import MaxMessageTermination, TextMentionTermination\n",
    "from autogen_agentchat.teams import RoundRobinGroupChat\n",
    "from autogen_agentchat.ui import Console\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "\n",
    "# model_client = OpenAIChatCompletionClient(\n",
    "#     model=\"gpt-4o\",\n",
    "#     temperature=1,\n",
    "#     # api_key=\"sk-...\", # Optional if you have an OPENAI_API_KEY env variable set.\n",
    "# )\n",
    "\n",
    "from autogen_core.models import ModelFamily\n",
    "model_client = OpenAIChatCompletionClient(\n",
    "    model=\"qwen2.5:7b\",\n",
    "    # model=\"llama3.2:3b\",\n",
    "    base_url=\"http://localhost:11434/v1\",\n",
    "    api_key=\"placeholder\",\n",
    "    model_info={\n",
    "        \"vision\": False,\n",
    "        \"function_calling\": True,\n",
    "        \"json_output\": False,\n",
    "        \"family\": ModelFamily.ANY,\n",
    "    },\n",
    "    temperature=1,\n",
    ")\n",
    "\n",
    "# Create the primary agent.\n",
    "primary_agent = AssistantAgent(\n",
    "    \"primary\",\n",
    "    model_client=model_client,\n",
    "    system_message=\"You are a helpful AI assistant.\",\n",
    ")\n",
    "\n",
    "# Create the critic agent.\n",
    "critic_agent = AssistantAgent(\n",
    "    \"critic\",\n",
    "    model_client=model_client,\n",
    "    system_message=\"Provide constructive feedback for every message. Respond with 'APPROVE' to when your feedbacks are addressed.\",\n",
    ")\n",
    "\n",
    "max_msg_termination = MaxMessageTermination(max_messages=3)\n",
    "round_robin_team = RoundRobinGroupChat([primary_agent, critic_agent], termination_condition=max_msg_termination)\n",
    "\n",
    "# Use asyncio.run(...) if you are running this script as a standalone script.\n",
    "await Console(round_robin_team.run_stream(task=\"Write a unique, Haiku about the weather in Paris\"), output_stats=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- primary ----------\n",
      "Rain kisses cobblestones,\n",
      "Eiffel shimmers through soft drizzle,\n",
      "Paris sighs, alive.\n",
      "[Prompt tokens: 98, Completion tokens: 23]\n",
      "---------- critic ----------\n",
      "Excellent! Your revisions enhance the Haiku by adding a sense of life and activity to the scene. Approve. APPROVE\n",
      "[Prompt tokens: 136, Completion tokens: 26]\n",
      "---------- primary ----------\n",
      "Approved!\n",
      "\n",
      "Rain kisses cobblestones,\n",
      "Eiffel shimmers through soft drizzle,\n",
      "Paris sighs, alive.\n",
      "[Prompt tokens: 155, Completion tokens: 25]\n",
      "---------- Summary ----------\n",
      "Number of messages: 3\n",
      "Finish reason: Maximum number of messages 3 reached, current message count: 3\n",
      "Total prompt tokens: 389\n",
      "Total completion tokens: 74\n",
      "Duration: 5.45 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TaskResult(messages=[TextMessage(source='primary', models_usage=RequestUsage(prompt_tokens=98, completion_tokens=23), metadata={}, content='Rain kisses cobblestones,\\nEiffel shimmers through soft drizzle,\\nParis sighs, alive.', type='TextMessage'), TextMessage(source='critic', models_usage=RequestUsage(prompt_tokens=136, completion_tokens=26), metadata={}, content='Excellent! Your revisions enhance the Haiku by adding a sense of life and activity to the scene. Approve. APPROVE', type='TextMessage'), TextMessage(source='primary', models_usage=RequestUsage(prompt_tokens=155, completion_tokens=25), metadata={}, content='Approved!\\n\\nRain kisses cobblestones,\\nEiffel shimmers through soft drizzle,\\nParis sighs, alive.', type='TextMessage')], stop_reason='Maximum number of messages 3 reached, current message count: 3')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use asyncio.run(...) if you are running this script as a standalone script.\n",
    "await Console(round_robin_team.run_stream(), output_stats=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- user ----------\n",
      "Write a unique, Haiku about the weather in Paris\n",
      "---------- primary ----------\n",
      "Mist wraps Eiffel Tower,\n",
      "Whispers through sleepy streets below,\n",
      "Paris breathes, alive.\n",
      "[Prompt tokens: 200, Completion tokens: 22]\n",
      "---------- critic ----------\n",
      "Your Haiku is beautifully crafted. To further enhance it, consider integrating the sensation of touch or smell to evoke more sensory details. APPROVE\n",
      "\n",
      "How about:\n",
      "Mist wraps Eiffel Tower,\n",
      "Whispers through sleepy streets below,\n",
      "Paris breathes, cold, sweet.\n",
      "[Prompt tokens: 204, Completion tokens: 56]\n",
      "---------- Summary ----------\n",
      "Number of messages: 3\n",
      "Finish reason: Text 'APPROVE' mentioned\n",
      "Total prompt tokens: 404\n",
      "Total completion tokens: 78\n",
      "Duration: 4.95 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TaskResult(messages=[TextMessage(source='user', models_usage=None, metadata={}, content='Write a unique, Haiku about the weather in Paris', type='TextMessage'), TextMessage(source='primary', models_usage=RequestUsage(prompt_tokens=200, completion_tokens=22), metadata={}, content='Mist wraps Eiffel Tower,\\nWhispers through sleepy streets below,\\nParis breathes, alive.', type='TextMessage'), TextMessage(source='critic', models_usage=RequestUsage(prompt_tokens=204, completion_tokens=56), metadata={}, content='Your Haiku is beautifully crafted. To further enhance it, consider integrating the sensation of touch or smell to evoke more sensory details. APPROVE\\n\\nHow about:\\nMist wraps Eiffel Tower,\\nWhispers through sleepy streets below,\\nParis breathes, cold, sweet.', type='TextMessage')], stop_reason=\"Text 'APPROVE' mentioned\")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_msg_termination = MaxMessageTermination(max_messages=10)\n",
    "text_termination = TextMentionTermination(\"APPROVE\")\n",
    "combined_termination = max_msg_termination | text_termination\n",
    "\n",
    "round_robin_team = RoundRobinGroupChat([primary_agent, critic_agent], termination_condition=combined_termination)\n",
    "\n",
    "# Use asyncio.run(...) if you are running this script as a standalone script.\n",
    "await Console(round_robin_team.run_stream(task=\"Write a unique, Haiku about the weather in Paris\"), output_stats=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- user ----------\n",
      "Write a unique, Haiku about the weather in Paris\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- primary ----------\n",
      "Rain whispers through spires,\n",
      "Seine murmurs with cobblestone mist,\n",
      "Paris breathes rain.\n",
      "[Prompt tokens: 31, Completion tokens: 22]\n",
      "---------- critic ----------\n",
      "That's beautiful! Here is another attempt:\n",
      "\n",
      "Gentle droplets dance,\n",
      "Over Louvre, they softly fall—\n",
      "Paris in soft rain.\n",
      "[Prompt tokens: 182, Completion tokens: 31]\n",
      "---------- primary ----------\n",
      "That one is lovely too!\n",
      "\n",
      "Here’s a third attempt if you'd like:\n",
      "\n",
      "Misty fog flows,\n",
      "Eiffel shrouded in ethereal veils—\n",
      "Paris in mist.\n",
      "[Prompt tokens: 92, Completion tokens: 39]\n",
      "---------- critic ----------\n",
      "Both are excellent! Here's your final crafted Haiku for the weather in Paris:\n",
      "\n",
      "Misty fog flows,\n",
      "Eiffel shrouded in veils—soft.\n",
      "Paris in mist.\n",
      "\n",
      "Please let me know if you need any adjustments or approval to proceed!\n",
      "[Prompt tokens: 260, Completion tokens: 53]\n",
      "---------- primary ----------\n",
      "Your Haiku is beautifully crafted and perfectly captures the essence of Parisian mist. Here's a slightly refined version for your satisfaction:\n",
      "\n",
      "Misty fog flows,\n",
      "Eiffel shrouded in veils—soft.\n",
      "Paris in mist.\n",
      "\n",
      "Feel free to use it as is, or let me know if you need any further adjustments!\n",
      "[Prompt tokens: 192, Completion tokens: 67]\n",
      "---------- critic ----------\n",
      "[FunctionCall(id='call_z1yfr2bc', arguments='{}', name='approve')]\n",
      "[Prompt tokens: 388, Completion tokens: 71]\n",
      "---------- critic ----------\n",
      "[FunctionExecutionResult(content='call approve function', name='approve', call_id='call_z1yfr2bc', is_error=False)]\n",
      "---------- critic ----------\n",
      "call approve function\n",
      "---------- Summary ----------\n",
      "Number of messages: 9\n",
      "Finish reason: Function 'approve' was executed.\n",
      "Total prompt tokens: 1145\n",
      "Total completion tokens: 283\n",
      "Duration: 18.82 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TaskResult(messages=[TextMessage(source='user', models_usage=None, metadata={}, content='Write a unique, Haiku about the weather in Paris', type='TextMessage'), TextMessage(source='primary', models_usage=RequestUsage(prompt_tokens=31, completion_tokens=22), metadata={}, content='Rain whispers through spires,\\nSeine murmurs with cobblestone mist,\\nParis breathes rain.', type='TextMessage'), TextMessage(source='critic', models_usage=RequestUsage(prompt_tokens=182, completion_tokens=31), metadata={}, content=\"That's beautiful! Here is another attempt:\\n\\nGentle droplets dance,\\nOver Louvre, they softly fall—\\nParis in soft rain.\", type='TextMessage'), TextMessage(source='primary', models_usage=RequestUsage(prompt_tokens=92, completion_tokens=39), metadata={}, content=\"That one is lovely too!\\n\\nHere’s a third attempt if you'd like:\\n\\nMisty fog flows,\\nEiffel shrouded in ethereal veils—\\nParis in mist.\", type='TextMessage'), TextMessage(source='critic', models_usage=RequestUsage(prompt_tokens=260, completion_tokens=53), metadata={}, content=\"Both are excellent! Here's your final crafted Haiku for the weather in Paris:\\n\\nMisty fog flows,\\nEiffel shrouded in veils—soft.\\nParis in mist.\\n\\nPlease let me know if you need any adjustments or approval to proceed!\", type='TextMessage'), TextMessage(source='primary', models_usage=RequestUsage(prompt_tokens=192, completion_tokens=67), metadata={}, content=\"Your Haiku is beautifully crafted and perfectly captures the essence of Parisian mist. Here's a slightly refined version for your satisfaction:\\n\\nMisty fog flows,\\nEiffel shrouded in veils—soft.\\nParis in mist.\\n\\nFeel free to use it as is, or let me know if you need any further adjustments!\", type='TextMessage'), ToolCallRequestEvent(source='critic', models_usage=RequestUsage(prompt_tokens=388, completion_tokens=71), metadata={}, content=[FunctionCall(id='call_z1yfr2bc', arguments='{}', name='approve')], type='ToolCallRequestEvent'), ToolCallExecutionEvent(source='critic', models_usage=None, metadata={}, content=[FunctionExecutionResult(content='call approve function', name='approve', call_id='call_z1yfr2bc', is_error=False)], type='ToolCallExecutionEvent'), ToolCallSummaryMessage(source='critic', models_usage=None, metadata={}, content='call approve function', type='ToolCallSummaryMessage')], stop_reason=\"Function 'approve' was executed.\")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Sequence\n",
    "\n",
    "from autogen_agentchat.base import TerminatedException, TerminationCondition\n",
    "from autogen_agentchat.messages import (\n",
    "    AgentEvent,\n",
    "    ChatMessage,\n",
    "    StopMessage,\n",
    "    ToolCallExecutionEvent,\n",
    ")\n",
    "from autogen_core import Component\n",
    "from pydantic import BaseModel\n",
    "from typing_extensions import Self\n",
    "\n",
    "\n",
    "class FunctionCallTerminationConfig(BaseModel):\n",
    "    \"\"\"Configuration for the termination condition to allow for serialization\n",
    "    and deserialization of the component.\n",
    "    \"\"\"\n",
    "\n",
    "    function_name: str\n",
    "\n",
    "\n",
    "class FunctionCallTermination(\n",
    "    TerminationCondition, Component[FunctionCallTerminationConfig]\n",
    "):\n",
    "    \"\"\"Terminate the conversation if a FunctionExecutionResult with a specific name is received.\"\"\"\n",
    "\n",
    "    component_config_schema = FunctionCallTerminationConfig\n",
    "    \"\"\"The schema for the component configuration.\"\"\"\n",
    "\n",
    "    def __init__(self, function_name: str) -> None:\n",
    "        self._terminated = False\n",
    "        self._function_name = function_name\n",
    "\n",
    "    @property\n",
    "    def terminated(self) -> bool:\n",
    "        return self._terminated\n",
    "\n",
    "    async def __call__(\n",
    "        self, messages: Sequence[AgentEvent | ChatMessage]\n",
    "    ) -> StopMessage | None:\n",
    "        if self._terminated:\n",
    "            raise TerminatedException(\"Termination condition has already been reached\")\n",
    "        for message in messages:\n",
    "            if isinstance(message, ToolCallExecutionEvent):\n",
    "                for execution in message.content:\n",
    "                    if execution.name == self._function_name:\n",
    "                        self._terminated = True\n",
    "                        return StopMessage(\n",
    "                            content=f\"Function '{self._function_name}' was executed.\",\n",
    "                            source=\"FunctionCallTermination\",\n",
    "                        )\n",
    "        return None\n",
    "\n",
    "    async def reset(self) -> None:\n",
    "        self._terminated = False\n",
    "\n",
    "    def _to_config(self) -> FunctionCallTerminationConfig:\n",
    "        return FunctionCallTerminationConfig(\n",
    "            function_name=self._function_name,\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def _from_config(cls, config: FunctionCallTerminationConfig) -> Self:\n",
    "        return cls(\n",
    "            function_name=config.function_name,\n",
    "        )\n",
    "\n",
    "\n",
    "def approve() -> str:\n",
    "    \"\"\"Approve the message when all feedbacks have been addressed.\"\"\"\n",
    "    return \"call approve function\"\n",
    "\n",
    "\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.teams import RoundRobinGroupChat\n",
    "from autogen_agentchat.ui import Console\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "\n",
    "# model_client = OpenAIChatCompletionClient(\n",
    "#     model=\"gpt-4o\",\n",
    "#     temperature=1,\n",
    "#     # api_key=\"sk-...\", # Optional if you have an OPENAI_API_KEY env variable set.\n",
    "# )\n",
    "\n",
    "from autogen_core.models import ModelFamily\n",
    "model_client = OpenAIChatCompletionClient(\n",
    "    model=\"qwen2.5:7b\",\n",
    "    # model=\"llama3.2:3b\",\n",
    "    base_url=\"http://localhost:11434/v1\",\n",
    "    api_key=\"placeholder\",\n",
    "    model_info={\n",
    "        \"vision\": False,\n",
    "        \"function_calling\": True,\n",
    "        \"json_output\": False,\n",
    "        \"family\": ModelFamily.ANY,\n",
    "    },\n",
    "    temperature=1,\n",
    ")\n",
    "\n",
    "# Create the primary agent.\n",
    "primary_agent = AssistantAgent(\n",
    "    \"primary\",\n",
    "    model_client=model_client,\n",
    "    system_message=\"You are a helpful AI assistant.\",\n",
    ")\n",
    "\n",
    "# Create the critic agent with the approve function as a tool.\n",
    "critic_agent = AssistantAgent(\n",
    "    \"critic\",\n",
    "    model_client=model_client,\n",
    "    tools=[approve],  # Register the approve function as a tool.\n",
    "    system_message=\"Provide constructive feedback. Use the approve tool to approve when all feedbacks are addressed.\",\n",
    ")\n",
    "\n",
    "function_call_termination = FunctionCallTermination(function_name=\"approve\")\n",
    "round_robin_team = RoundRobinGroupChat([primary_agent, critic_agent], termination_condition=function_call_termination)\n",
    "\n",
    "# Use asyncio.run(...) if you are running this script as a standalone script.\n",
    "await Console(round_robin_team.run_stream(task=\"Write a unique, Haiku about the weather in Paris\"), output_stats=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "open_manus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
