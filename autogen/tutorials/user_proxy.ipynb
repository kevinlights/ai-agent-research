{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- user ----------\n",
      "Write a 4-line poem about the ocean.\n",
      "---------- assistant ----------\n",
      "Here's a 4-line poem about the ocean:\n",
      "\n",
      "Deep blue waves crash on the shore,\n",
      "A soothing melody forever more.\n",
      "The ocean's vastness touches the sky,\n",
      "A sight to cherish, as the tide goes by.\n",
      "---------- user_proxy ----------\n",
      "APPROVE\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TaskResult(messages=[TextMessage(source='user', models_usage=None, metadata={}, content='Write a 4-line poem about the ocean.', type='TextMessage'), TextMessage(source='assistant', models_usage=RequestUsage(prompt_tokens=59, completion_tokens=46), metadata={}, content=\"Here's a 4-line poem about the ocean:\\n\\nDeep blue waves crash on the shore,\\nA soothing melody forever more.\\nThe ocean's vastness touches the sky,\\nA sight to cherish, as the tide goes by.\", type='TextMessage'), UserInputRequestedEvent(source='user_proxy', models_usage=None, metadata={}, request_id='2f742cc6-2528-455e-9acb-a88f59ac9f8f', content='', type='UserInputRequestedEvent'), TextMessage(source='user_proxy', models_usage=None, metadata={}, content='APPROVE', type='TextMessage')], stop_reason=\"Text 'APPROVE' mentioned\")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from autogen_agentchat.agents import AssistantAgent, UserProxyAgent\n",
    "from autogen_agentchat.conditions import TextMentionTermination\n",
    "from autogen_agentchat.teams import RoundRobinGroupChat\n",
    "from autogen_agentchat.ui import Console\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "\n",
    "# Create the agents.\n",
    "# model_client = OpenAIChatCompletionClient(model=\"gpt-4o-mini\")\n",
    "from autogen_core.models import ModelFamily\n",
    "model_client = OpenAIChatCompletionClient(\n",
    "    # model=\"qwen2.5:7b\",\n",
    "    model=\"llama3.2:3b\",\n",
    "    base_url=\"http://localhost:11434/v1\",\n",
    "    api_key=\"placeholder\",\n",
    "    model_info={\n",
    "        \"vision\": False,\n",
    "        \"function_calling\": True,\n",
    "        \"json_output\": False,\n",
    "        \"family\": ModelFamily.ANY,\n",
    "    },\n",
    ")\n",
    "\n",
    "assistant = AssistantAgent(\"assistant\", model_client=model_client)\n",
    "user_proxy = UserProxyAgent(\"user_proxy\", input_func=input)  # Use input() to get user input from console.\n",
    "\n",
    "# Create the termination condition which will end the conversation when the user says \"APPROVE\".\n",
    "termination = TextMentionTermination(\"APPROVE\")\n",
    "\n",
    "# Create the team.\n",
    "team = RoundRobinGroupChat([assistant, user_proxy], termination_condition=termination)\n",
    "\n",
    "# Run the conversation and stream to the console.\n",
    "stream = team.run_stream(task=\"Write a 4-line poem about the ocean.\")\n",
    "# Use asyncio.run(...) when running in a script.\n",
    "await Console(stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- user ----------\n",
      "Write a 4-line poem about the ocean.\n",
      "---------- assistant ----------\n",
      "Waves crash on the sandy shore,\n",
      "The ocean's roar, forever more.\n",
      "Deep blue waters, endless seas,\n",
      "A mystery that's full of glee.\n",
      "\n",
      " Terminated.\n",
      "[Prompt tokens: 59, Completion tokens: 36]\n",
      "---------- Summary ----------\n",
      "Number of messages: 2\n",
      "Finish reason: Maximum number of turns 1 reached.\n",
      "Total prompt tokens: 59\n",
      "Total completion tokens: 36\n",
      "Duration: 0.97 seconds\n",
      "---------- user ----------\n",
      "Can you make it about a person and its relationship with the ocean\n",
      "---------- assistant ----------\n",
      "Here is a 4-line poem:\n",
      "\n",
      "Tides of solitude, I find my home\n",
      "Where waves caress the soul, I am not alone\n",
      "In the depths below, I see a truth revealed\n",
      "A connection that within me forever dwells.\n",
      "\n",
      "Terminated.\n",
      "[Prompt tokens: 117, Completion tokens: 54]\n",
      "---------- Summary ----------\n",
      "Number of messages: 2\n",
      "Finish reason: Maximum number of turns 1 reached.\n",
      "Total prompt tokens: 117\n",
      "Total completion tokens: 54\n",
      "Duration: 1.49 seconds\n",
      "---------- user ----------\n",
      "good\n",
      "---------- assistant ----------\n",
      "I'm glad you liked it! If you want another one or have any specific requests, feel free to ask!\n",
      "[Prompt tokens: 181, Completion tokens: 24]\n",
      "---------- Summary ----------\n",
      "Number of messages: 2\n",
      "Finish reason: Maximum number of turns 1 reached.\n",
      "Total prompt tokens: 181\n",
      "Total completion tokens: 24\n",
      "Duration: 0.86 seconds\n"
     ]
    }
   ],
   "source": [
    "# Once the team stops, the turn count will be reset. When you resume the team, it will start from 0 again. However, the teamâ€™s internal state will be preserved, for example, the RoundRobinGroupChat will resume from the next agent in the list with the same conversation history.\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.teams import RoundRobinGroupChat\n",
    "from autogen_agentchat.ui import Console\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "\n",
    "# Create the agents.\n",
    "# model_client = OpenAIChatCompletionClient(model=\"gpt-4o-mini\")\n",
    "from autogen_core.models import ModelFamily\n",
    "model_client = OpenAIChatCompletionClient(\n",
    "    # model=\"qwen2.5:7b\",\n",
    "    model=\"llama3.2:3b\",\n",
    "    base_url=\"http://localhost:11434/v1\",\n",
    "    api_key=\"placeholder\",\n",
    "    model_info={\n",
    "        \"vision\": False,\n",
    "        \"function_calling\": True,\n",
    "        \"json_output\": False,\n",
    "        \"family\": ModelFamily.ANY,\n",
    "    },\n",
    ")\n",
    "\n",
    "assistant = AssistantAgent(\"assistant\", model_client=model_client)\n",
    "\n",
    "# Create the team setting a maximum number of turns to 1.\n",
    "team = RoundRobinGroupChat([assistant], max_turns=1)\n",
    "\n",
    "task = \"Write a 4-line poem about the ocean.\"\n",
    "while True:\n",
    "    # Run the conversation and stream to the console.\n",
    "    stream = team.run_stream(task=task)\n",
    "    # Use asyncio.run(...) when running in a script.\n",
    "    await Console(stream, output_stats=True)\n",
    "    # Get the user response.\n",
    "    task = input(\"Enter your feedback (type 'exit' to leave): \")\n",
    "    if task.lower().strip() == \"exit\":\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- user ----------\n",
      "What is the weather in New York today?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- lazy_assistant ----------\n",
      "[FunctionCall(id='call_u2rtvpx1', arguments='{}', name='transfer_to_user')]\n",
      "[Prompt tokens: 167, Completion tokens: 84]\n",
      "---------- lazy_assistant ----------\n",
      "[FunctionExecutionResult(content='Transfer to user.', name='transfer_to_user', call_id='call_u2rtvpx1', is_error=False)]\n",
      "---------- lazy_assistant ----------\n",
      "Transfer to user.\n",
      "---------- Summary ----------\n",
      "Number of messages: 4\n",
      "Finish reason: Handoff to user from lazy_assistant detected.\n",
      "Total prompt tokens: 167\n",
      "Total completion tokens: 84\n",
      "Duration: 5.30 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TaskResult(messages=[TextMessage(source='user', models_usage=None, metadata={}, content='What is the weather in New York today?', type='TextMessage'), ToolCallRequestEvent(source='lazy_assistant', models_usage=RequestUsage(prompt_tokens=167, completion_tokens=84), metadata={}, content=[FunctionCall(id='call_u2rtvpx1', arguments='{}', name='transfer_to_user')], type='ToolCallRequestEvent'), ToolCallExecutionEvent(source='lazy_assistant', models_usage=None, metadata={}, content=[FunctionExecutionResult(content='Transfer to user.', name='transfer_to_user', call_id='call_u2rtvpx1', is_error=False)], type='ToolCallExecutionEvent'), HandoffMessage(source='lazy_assistant', models_usage=None, metadata={}, target='user', content='Transfer to user.', context=[], type='HandoffMessage')], stop_reason='Handoff to user from lazy_assistant detected.')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.base import Handoff\n",
    "from autogen_agentchat.conditions import HandoffTermination, TextMentionTermination\n",
    "from autogen_agentchat.teams import RoundRobinGroupChat\n",
    "from autogen_agentchat.ui import Console\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "\n",
    "# Create an OpenAI model client.\n",
    "# model_client = OpenAIChatCompletionClient(\n",
    "#     model=\"gpt-4o\",\n",
    "#     # api_key=\"sk-...\", # Optional if you have an OPENAI_API_KEY env variable set.\n",
    "# )\n",
    "from autogen_core.models import ModelFamily\n",
    "model_client = OpenAIChatCompletionClient(\n",
    "    model=\"qwen2.5:7b\",\n",
    "    # model=\"llama3.2:3b\",\n",
    "    base_url=\"http://localhost:11434/v1\",\n",
    "    api_key=\"placeholder\",\n",
    "    model_info={\n",
    "        \"vision\": False,\n",
    "        \"function_calling\": True,\n",
    "        \"json_output\": False,\n",
    "        \"family\": ModelFamily.ANY,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Create a lazy assistant agent that always hands off to the user.\n",
    "lazy_agent = AssistantAgent(\n",
    "    \"lazy_assistant\",\n",
    "    model_client=model_client,\n",
    "    handoffs=[Handoff(target=\"user\", message=\"Transfer to user.\")],\n",
    "    # system_message=\"If you cannot complete the task, transfer to user. Otherwise, when finished, respond with 'TERMINATE'.\",\n",
    "    system_message=\"If you cannot complete the task or tool is not available, transfer to user. Otherwise, when finished with good data, respond with 'TERMINATE'.\",\n",
    ")\n",
    "\n",
    "# Define a termination condition that checks for handoff messages.\n",
    "handoff_termination = HandoffTermination(target=\"user\")\n",
    "# Define a termination condition that checks for a specific text mention.\n",
    "text_termination = TextMentionTermination(\"TERMINATE\")\n",
    "\n",
    "# Create a single-agent team with the lazy assistant and both termination conditions.\n",
    "lazy_agent_team = RoundRobinGroupChat([lazy_agent], termination_condition=handoff_termination | text_termination)\n",
    "\n",
    "# Run the team and stream to the console.\n",
    "task = \"What is the weather in New York today?\"\n",
    "await Console(lazy_agent_team.run_stream(task=task), output_stats=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- user ----------\n",
      "The weather in New York is sunny.\n",
      "---------- lazy_assistant ----------\n",
      "TERMINATE\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TaskResult(messages=[TextMessage(source='user', models_usage=None, metadata={}, content='The weather in New York is sunny.', type='TextMessage'), TextMessage(source='lazy_assistant', models_usage=RequestUsage(prompt_tokens=218, completion_tokens=4), metadata={}, content='TERMINATE', type='TextMessage')], stop_reason=\"Text 'TERMINATE' mentioned\")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await Console(lazy_agent_team.run_stream(task=\"The weather in New York is sunny.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "open_manus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
